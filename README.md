# Laboratorium 5: Deployment, Quota i LimitRange w Kubernetes

Repozytorium zawiera pliki konfiguracyjne YAML oraz dokumentacj zada wykonanych w ramach Laboratorium nr 5 (PFSwChO).

Celem wicze byo zapoznanie si z zaawansowanymi metodami zarzdzania zasobami (`ResourceQuota`, `LimitRange`) oraz sposobem dziaania obiekt贸w `Deployment`.

##  Cz 1: wiczenia z Instrukcji

### [cite_start]Zarzdzanie zasobami (`appresources.yaml`) [cite: 28-33]
* **Krok 1:** Poprawne uruchomienie Poda `myapp` (MySQL + WordPress) z domylnymi zasobami.
* [cite_start]**Krok 2 (Bd):** Modyfikacja pliku `appresources.yaml` i zmiana `requests: memory` na `64Gi`[cite: 31].
* **Diagnoza:** Pod "utkn" w stanie `Pending`. Polecenie `kubectl describe pod myapp` wykazao bd `FailedScheduling` z powodu `Insufficient memory`. Klaster nie m贸g zagwarantowa 64GB RAM.

### [cite_start]Quota i Deployment (`limiter`) [cite: 53-59, 81, 101]
* [cite_start]**Krok 1:** Utworzono `namespace restricted` oraz `ResourceQuota` o nazwie `xlimits` (limit `cpu=1`, `memory=500M`, `pods=3`) [cite: 53-59].
* [cite_start]**Krok 2 (Bd):** Pr贸ba uruchomienia `Deployment` o nazwie `limiter` (`kubectl create deploy limiter...`) [cite: 74-76] nie powioda si (`READY: 0/3`). [cite_start]Diagnoza: `Quota` zablokowaa tworzenie Pod贸w, poniewa偶 Deployment *nie definiowa* `requests` ani `limits` dla zasob贸w[cite: 39].
* [cite_start]**Krok 3 (Naprawa):** Limity i 偶dania zostay dodane "w locie" za pomoc polecenia `kubectl set resources deployment limiter --requests=... --limits=...` [cite: 92-93, 101-104].
* **Krok 4 (Sukces):** Deployment poprawnie uruchomi 3 Pody. Polecenie `kubectl describe quota xlimits` pokazao zu偶ycie zasob贸w (np. `cpu: 300m/1`).

### Monitorowanie i Edycja (`kubectl top`, `kubectl edit`)
* **Problem:** Polecenie `kubectl top` zwr贸cio bd `Metrics API not available`.
* [cite_start]**Rozwizanie:** Wczenie dodatku `minikube addons enable metrics-server` rozwizao problem i pozwolio monitorowa bie偶ce zu偶ycie CPU/RAM przez Pody [cite: 156-161].
* [cite_start]**Edycja:** Za pomoc `kubectl edit quota xlimits` [cite: 177-179] limit CPU zosta obni偶ony do `100m`. [cite_start]Spowodowao to, 偶e zu偶ycie przekroczyo limit (`300m/100m`), jednak Pody **nadal dziaay**, co dowodzi, 偶e Quota jest sprawdzana *tylko w momencie tworzenia* Poda [cite: 239-240].

### [cite_start]LimitRange (`limitrangetest.yaml`) [cite: 244-246, 258]
* [cite_start]**Krok 1:** Utworzono `namespace galaxy` [cite: 253] [cite_start]oraz `LimitRange` [cite: 258-262][cite_start], kt贸ry ustawia domylne zasoby (Request: 256Mi, Limit: 512Mi) dla kontener贸w[cite: 260].
* [cite_start]**Krok 2 (Test):** Uruchomiono Poda `nginx` (`kubectl run lmemory...`) *bez* definiowania zasob贸w [cite: 269-270].
* **Diagnoza:** `kubectl describe pod lmemory` pokazao, 偶e Pod **automatycznie otrzyma** `Requests: memory: 256Mi` oraz `Limits: memory: 512Mi`, zgodnie z definicj w `LimitRange`.

### [cite_start]Obiekt Deployment (Waciwoci) [cite: 273, 280, 287-292]
* [cite_start]**Samo-naprawianie:** Po rcznym usuniciu Poda z Deploymentu (`kubectl delete pod...`), `ReplicaSet` natychmiast wykry brak i uruchomi nowy Pod, aby utrzyma 偶dan liczb replik [cite: 280-281].
* [cite_start]**Skalowanie:** Pokazano dwie metody skalowania Deploymentu `dredis`: za pomoc `kubectl scale deploy dredis --replicas=4` [cite: 288] [cite_start]oraz `kubectl edit deploy dredis` [cite: 291-292].

---

##  Cz 2: Zadanie G贸wne (ns-dev, ns-prod)

[cite_start]Zgodnie z zadaniem [cite: 296-306], skonfigurowano dwa rodowiska.

* **`ns-prod`:** Otrzyma `ResourceQuota` z wysokimi limitami zasob贸w (plik `prod-quota.yaml`).
* [cite_start]**`ns-dev`:** Otrzyma `ResourceQuota` (limit `pods=10`) oraz `LimitRange` (max `cpu=200m`, `memory=256Mi`) (plik `dev-setup.yaml`) [cite: 298-299].

### Testowanie `ns-dev`:

1.  **`zero-test` (Test 4):**
    * [cite_start]**Akcja:** `kubectl create deploy zero-test --image=nginx -n ns-dev`[cite: 306].
    * **Wynik:** **Sukces**. Pod uruchomi si poprawnie.
    * **Dow贸d:** `kubectl describe pod...` wykaza, 偶e Pod automatycznie otrzyma domylne limity (200m CPU / 256Mi RAM) z `LimitRange`.

2.  **`no-test` (Test 2):**
    * [cite_start]**Akcja:** Pr贸ba ustawienia limit贸w `cpu=300m` (`kubectl set resources...`), co przekracza limit `LimitRange` (200m)[cite: 304].
    * **Wynik:** **Bd**. Deployment nie by w stanie utworzy Poda (`READY: 0/1`).
    * **Dow贸d:** `kubectl describe replicaset...` pokaza bd `FailedCreate` z komunikatem, 偶e 偶dane zasoby (`300m`) przekraczaj maksymalny limit `LimitRange` (`200m`).

3.  **`yes-test` (Test 3):**
    * [cite_start]**Akcja:** Ustawienie limit贸w `cpu=150m`, czyli *poni偶ej* limitu `LimitRange`[cite: 305].
    * **Wynik:** **Sukces**. Pod uruchomi si poprawnie, poniewa偶 mieci si w dozwolonych limitach.
